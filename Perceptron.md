# 感知机（Perceptron）

## 一、背景

​        感知机是Frank Rosenblatt在1957年于康奈尔航空实验室（Cornell Aeronautical Laboratory）时发明的一种[人工神经网络](https://wanweibaike.net/wiki-人工神经网络)。它可以被视为一种最简单形式的[前馈神经网络](https://wanweibaike.net/wiki-前馈神经网络)，是一种二元[线性分类器](https://wanweibaike.net/wiki-线性分类器)，是**神经网络和支持向量机**的基础。

## 二、定义原理

​        在背景中提到，感知机是一种二元线性分类器，通俗地说，就是感知机能够实现：将n维数据集先进行“是/非”仅两类的类型判别，再寻求一个超平面，使得这两类数据点能够<u>完全地、正确地</u>被这个超平面<u>线性地</u>一分为二，当然前提是数据集线性可分。例如下图，一条直线将二维平面上两种类别的数据点完全地一分为二：

​                                                                      <img src="https://cdn.jsdelivr.net/gh/KevinHaoo/PicCloud/WEB393a13769317374240437edb5e1d3b26.png" alt="" style="zoom:33%;" />      

### 2.1 一些概念的引入及其推广

#### 2.1.1 点到直线距离

​        设直线方程为 $𝐴𝑥+𝐵𝑦+𝐶 = 0$，点P的坐标为 $(x_0,y_0)$。 那么，P到直线距离为：
$$
d=\frac{Ax+By+C}{\sqrt{A^2+B^2}}
$$

#### 2.1.2 点到超平面距离（由2.1.1推广）

​        由于在高维空间中，

​        设超平面为 $ℎ = 𝑤𝑥+𝑏$，其中 $w = (w0,w1,...wn)，x = (x0,x1,...xn)$，样本点 $x$ 到超平面的距离如下：
$$
d=\frac{𝑤x+b}{||𝑤||}
$$

#### 2.1.3 超平面（Hyperplanes）

​        超平面是在 $n$ 维空间内的一个 $n-1$ 维的子空间，它能够将这个 $n$ 维空间线性地一分为二。例如，2 维空间中的超平面是一条线，3 维空间中的超平面是一个平面。

### 2.2 感知机模型的建立

#### 2.2.1 引例

​        不妨由一个具体例子来更直观地引入模型：

> ​        对于银行是否给用户发信用卡的问题，银行会根据顾客的个人信息来判断是否发放信用卡。此时，将顾客抽象为一个向量 X（包括姓名、年龄、年收入、负债数等特征变量）。同时设定各个属性所占的权重向量 W，对于正相关的属性设置相对较高的比例，如年收入。对于负相关的属性设置较低的比例，如负债数。y表示是否想该用户发放了信用卡。
>
> ​        通过求 X 和 W 的内积再减去一个“阀值”，若为正则同意发放信用卡，否则不发放信用卡。我们假设存在着一个从 X 到 Y 的映射 f ，感知学习算法（PLA）就是用来模拟这个映射，使得求出的函数与 f 尽可能的相似，起码在已知的数据集上一致。 

​        **从这个例子可以总结：**

1. **感知机的作用在于，既能够将已有的数据集线性分为两类，又对于未知的数据具有较准确的二元线性分类的能力。**
2. **感知机的分类结果一定是将数据集分成总体特征水平不同的两类。**
3. **感知机分类标准的建立是相对的，是可以人工制定的（但是需要保证数据集在这个标准上线性可分）。而如果没有事先制定分类标准，是可以利用感知学习算法求出可能成立的分类标准的（例如已知上图中的数据点从而求得图中红色直线）。**



​        由此，可以清楚地建立感知机的数学理论模型。

#### 2.2.2 理论模型阐述

​        设一数据集 $X$ ，而 $X$ 中存在 $n$ 维的特征变量。

​        设对于 $X$ ，存在由 $X$ 中的 $n$ 维特征变量经过一个映射 $f$ 得到的因变量 $f(x)$ ，并设  $f(x) = 𝑤x+b$.

​        其中， 𝑤 是各部分特征的权重向量，一般通过迭代求出的较优值， *x* 是各部分的特征变量构成的向量，*b* 是偏置（即一个不依赖于任何输入值的常数，可以认为是激励函数的偏移量，或者给神经元一个基础活跃等级，偏置改变了决策边界的位置，例如上面例子的“阀值”）, 𝑤x是点积。

​        规定：若 $f(x) \geq 0$，则输出 1；若 $f(x) < 0$，则输出 -1. 

​        即：
$$
f(x) = 𝑤x+b\\
y = sign[f(x)] = \begin{cases} 
1,  & \text f(x)\geq0 \\
-1, & \text f(x)<0\end{cases}
$$
​        

​        而对于这个映射 *f*，由 2.1.2 的“点到超平面距离“定义，不难发现此处的 $f(x)$ 正是点 $x = (x_0,x_1,...x_n)$ 到一个超平面的距离表达式，或者也可以说成是将线性函数 $f(x) = 𝑤x+b$ 具象为点到超平面的距离（这种具象方法或许可以推广以便于解决其他问题)。

​        然而，通常情况下，权重关系函数（即超平面方程）是未知的，仅有的条件是已分类好的、且具有数个维度的特征数据的样本。

​        这时，需要解决的问题便是：求得一个权重关系函数（即一个超平面），使得数据集能在此权重关系函数的变换下，其函数值满足大于等于零时为一类，小于零时为另一类，总体上有且只有两类。

​        要想求出这个权重关系函数，需要确定各维度系数 𝑤 和偏置 *b* （与引例不同，这里是为了使数据被分类，从而引人超平面方程，所以两个参数均是需要调优的）。这里便引入损失函数，用来作为选择两个参数的依据，或者说是优化的目标。

#### 2.2.3 损失函数

​        为了将误差定量地表示出来，定义如下的损失函数：
$$
Lo s s(𝑤,b)=-\frac{1}{||𝑤||}\sum_{x_i\in M}{y_i(𝑤x_i+b)}
$$
​        其中，M 集合是误分类点的集合。

​        不考虑 $\frac{1}{||𝑤||}$ ，就得到更简洁的感知机模型损失函数：

$$
Loss(𝑤,b)=-\sum_{x_i\in M}{y_i(𝑤x_i+b)}
$$



 **· 补充说明：为什么可以不考虑 $\frac{1}{||𝑤||}$**  ？

1. $\frac{1}{||𝑤||}$ 不影响 $y_i(𝑤x_i+b)$ 正负的判断，即不影响学习算法的中间过程。

   因为感知机学习算法是误分类驱动的，这里需要注意的是，所谓“误分类驱动”，指的是我们只需要判断 $−𝑦_i(𝑤𝑥_𝑖+𝑏)$ 的正负来判断分类的正确与否，而 $\frac{1}{||𝑤||}$ 并不影响正负值的判断。所以 $\frac{1}{||𝑤||}$ 对感知机学习算法的中间过程可以不考虑。

2. $\frac{1}{||𝑤||}$ 不影响感知机学习算法的最终结果。

   因为感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点，则此时损失函数为 0. 对应于 $-\frac{1}{||𝑤||}\sum_{x_i\in M}{y_i(𝑤x_i+b)}$ ，即分子为 0. 则可以看出 $\frac{1}{||𝑤||}$ 对最终结果也无影响。

​        综上，即使忽略 $\frac{1}{||𝑤||}$ ，也不会对感知机学习算法的执行过程产生任何影响，反而还能简化运算，提高算法执行效率。



## 三、感知机学习算法

​        感知机学习算法是对上述损失函数进行极小化，求得 𝑤 和 𝑏 。

​        但是，用一般基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的。原因在于，我们的损失函数里面有限定：<u>只有误分类的M集合里面的样本才能参与损失函数的优化</u>。

​        所以，我们不能用最普通的批量梯度下降，只能采用 **随机梯度下降**（SGD），其基石是泰勒公式，逐步推导得出，要沿着损失函数的负梯度方向前进才能找到更小的函数值。具体数学原理可参考文献 [5]。

​        目标函数如下：
$$
Target(𝑤,𝑏)=argmin_{𝑤,b}(−\sum_{x_i∈𝑀}y_𝑖(𝑤𝑥_𝑖+𝑏))
$$
​        其中， $arg min_{𝑤,b}$ 表示：当括号内的损失函数取最小值时，*𝑤 和 b* 的取值。

### 3.1 梯度下降法

​        简要介绍一下梯度下降法$^{[5]}$。

​        梯度下降法(gradient descent)，是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。我们来考虑一个无约束优化问题 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmin_x+f%28x%29) , 其中 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 为连续可微函数，如果我们能够构造一个序列 $x_0,x_1,x_2...$ ，并能够满足：
$$
f(x_{t+1})<f(x_t),\ t=0,1,2,...
$$
​        那么，我们就能够不断执行该过程，最终便可收敛到 **局部极小点**，可参考下图：

<img src="https://cdn.jsdelivr.net/gh/KevinHaoo/PicCloud/v2-e1e6b238b5292251690526c055858fc6_1440w.jpg" alt="" style="zoom:33%;" />      

​        接下来的问题就是：如何找到下一个点 $x_{t+1}$ ，并保证 $f(x_{t+1})<f(x_t)$ 呢？假设我们当前的函数 $f(x)$ 的形式是上图的形状，现在我们随机找了一个初始的点 $x_1$ ，对于一元函数来说，函数值只会随着 $x$ 的变化而变化，那么我们就设计**下一个 $x_{t+1}$ 是从上一个 $x_t$ 沿着某一方向走一小步 $\Delta x$ 得到的。此处的关键问题就是：这一小步的方向是朝向哪里？**

​        根据梯度的定义：梯度表示一个方向，这个方向是所探讨的点在所探讨的空间几何面上面上升最快的方向，而**负梯度**则相反地表示为**下降最快的方向**$^{[6]}$。

​        

​        根据梯度定义，分别对 $𝑤,𝑏$ 求偏导数，从而计算出梯度：
$$
\nabla_\omega L(\omega,b)=-\sum_{x_i\in M}y_ix_i\\
\nabla_b L(\omega,b)=-\sum_{x_i\in M}y_i
$$

### 3.2 感知机初始算法

​        **利用梯度的思想，选取某一当前权重函数下的误分类点，迭代 $𝑤,𝑏$ 的值，使这个点在损失函数里代入值为零，然后继续向下一个数据点遍历，直到所有的数据点都满足损失函数里代入值为零这个条件。**

（**注意**：上述算法思想仅仅适用于数据集满足线性可分的情况，对于线性不可分的数据来说，这个算法会无限循环下去，这个也是感知机算法的局限性。因此有了后人提出的 **Pocket Algorithm（口袋算法）**，这个算法目标是对于 $\omega,b$ 的迭代，先设定好**最大迭代的次数**，如50000次，再进行迭代优化，如果迭代时的 $\omega,b$ 使得损失函数比之前的要小，则更新参数值，否则不更新。当到了最大迭代次数，则程序结束。这么做的优势在于，我们不需要找的一个完美的结果，我们**仅仅需要找的一个相对来说较优的结果**即可；同时劣势在于，$\omega,b$ 是随机的，可能算了很多次都没有找出一个更好的，另外如果数据一开始就是线性可分，那么这个算法找出来的未必是最优解，且花费时间可能较大。）



​        下面列出系统的感知机初始算法过程：



​        <u>**输入**</u>：训练数据集 $D=(𝑥_1,𝑦_1),(𝑥_2,𝑦_2),...,(𝑥_n,𝑦_n).$  其中，$\ 𝑦_𝑖∈(−1,+1)$，

​                    学习率 $𝜂\ (0<𝜂\leq1)$

​        <u>**输出**</u>：𝑤, 𝑏 ;  感知机模型 $𝑓(𝑥)=𝑠𝑖𝑔𝑛(\sum_{i=1}^n{\eta_iy_i𝑥_i·x+𝑏})$



​        每次迭代的权重向量以如下方式更新：

​        **Step 1**  赋初值 $𝑤_0,𝑏_0$.

​        **Step 2**  选取数据点 $(𝑥_𝑖,𝑦_𝑖)$.

​        **Step 3**  判断该数据点是否为当前模型的误分类点，若 $𝑦_𝑖(\sum_{i=1}^n{\eta_iy_i𝑥_i·x+𝑏})\leq0$ ，则更新.

​        **Step 4**  对于每个 $D_{m}=\{(x_{1},y_{1}),\dots ,(x_{m},y_{m})\}$ 中的每个 $(x,y)$ 对，进行以下迭代：
$$
\omega_i = \omega_i+\eta y_ix_i
\\𝑏_i=𝑏_i+𝜂𝑦_i
$$
​        其中，$i∈(1,n)\bigcap Z$ .

​        **Step 5**  转至 Step 2，直至没有误分类点.

### 3.3 感知机对偶算法

​        对偶是优化理论里的问题。通常情况下，在某种条件下，原问题可以转化为等价的更容易求解的对偶问题。这里不具体展开，主要借助于**拉格朗日对偶性**相关理论.

​        下面对对偶算法作简单梳理：

​        因为感知机算法是误分类点驱动的，所以，对模型参数更新有贡献的数据一定是被误分类的点。假设样本点 ![[公式]](https://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29) 在整个更新过程中被误分类![[公式]](https://www.zhihu.com/equation?tex=n_i)次，则式 $\omega_i = \omega_i+\eta y_ix_i\, \\𝑏_i=𝑏_i+𝜂𝑦_i$ 可以表示为：
$$
\omega=\sum_{i=1}^N{n_i\eta y_ix_i}
\\b=\sum_{i=1}^N{n_i\eta y_i}
$$
​        $n_i$ 越大，表明第 $i$ 个数据点离最终要求的超平面越近，超平面只要稍微动一下，该点就从正样本变成了负样本或者相反，这样的点往往对最终的超平面影响越大。

​        将上面的迭代式代入到感知机模型中进行迭代，得到：
$$
f(x)=sign(\sum_{j=1}^N{n_i\eta y_ix_i}·x+\sum_{j=1}^N{n_i\eta y_i})
$$
​        **这时，模型中的参数由原来的 $\omega$ 和 $b$ 变成了 $n_i$** .



​        与感知机原始算法相对应，则有：

​        <u>**输入**</u>：训练数据集 $D=(𝑥_1,𝑦_1),(𝑥_2,𝑦_2),...,(𝑥_n,𝑦_n).$  其中，$\ 𝑦_𝑖∈(−1,+1)$，

​                    学习率 $𝜂\ (0<𝜂\leq1)$

​        <u>**输出**</u>：𝑤, 𝑏 ;  感知机模型 $f(x)=sign(\sum_{j=1}^N{n_i\eta y_ix_i}·x+\sum_{j=1}^N{n_i\eta y_i})$



​        每次迭代的权重向量以如下方式更新：

​        **Step 1**  赋初值 $n_i=0$.

​        **Step 2**  选取数据点 $(𝑥_𝑖,𝑦_𝑖)$.

​        **Step 3**  出现误分类点，即 $𝑦_𝑖(\sum_{i=1}^n{\eta_iy_i𝑥_i·x+𝑏})\leq0$时 ，则进行 Step 4 的更新.

​        **Step 4**  对于每个 $D_{m}=\{(x_{1},y_{1}),\dots ,(x_{m},y_{m})\}$ 中的每个 $(x,y)$ 对，进行以下迭代：
$$
n_i=n_i+1
$$
​        其中，$i∈(1,n)\bigcap Z$ .

​        **Step 5**  转至 Step 2，直至没有误分类点.

​        从以上推导可以看出，感知机的原始形式和对偶形式本质上一样，但从具体的计算过程来看，数据点仅以向量内积的形式出现。也就是说，可以预先计算数据点之间的内积并以矩阵的形式存储，则这个矩阵就是Gram矩阵。
$$
G=[x_i·x_j]_{M\times N}
$$
​        如此，每次做误分类点检测时候只需要在 $Garm$ 矩阵里查找就能获取内积 $[x_i·x_j]$ ，所以这个误分类点检测的时间复杂度是  $O(1)$。也就是说，对偶形式的感知机算法，把每轮迭代的时间复杂度从特征空间维度 $n$ 变成了样本数据数量 $N$.

### 3.4  PLA改进 --- Pocket Algorithm（口袋算法）

​        思路是：能否找到一条分界线去分离两个数据集，使得错误率最低呢？

​        要准确找到一条错误率最低的，被证明出是一个 NP 问题，不能准确地找到，只有满足训练集的权重函数。

​        那么，Pocket算法的思路就是（类似贪心），如果找到一个更好的（错误率低） $w$，那么就去更新，如果找一个错误率更高的，那么就不去更新的，重新遍历。

​        每次找的是一个随机的 $w$。对于所有数据都采用 $w'=w+\eta y_i·x_i$ 修正一次。最后如果这个 $w'$ 要比记录的最好值 $w$ 犯的错误少，就更新最好值。

​        但是，Pocket 算法缺陷是花费的时间更多，因为 PLA 原始算法只需要证明一个点犯错就表示权重函数不合适。然而，Pocket 算法还要额外把所有犯错的点都找出来，并相加对比，所以花费时间更多，时间成本更大，遇到一些数据集可能会出现运行一整晚都没能出结果的情况。

## 四、算法实现

```c++
#include<iostream>
#include<vector>
using namespace std;

//特征的维度
int n;
double eta = 0.1;

//以二维空间为例，x1 x2为特征变量
struct Variable{
    int x0;
    double x1, x2;
    int label;
};
//权重结构体，w1 w2 为属性 x1 x2 的权重
//字母后面的数字为特征维度，最大值即为该数据维度，初始值全设为 0
struct Weight{
    double w[3];
}Wit0 = {0, 0, 0};

//符号函数，根据向量内积和的特点判断 ”是/否“
int sign(double x){
    if(x>=0) return 1;
    else    return -1;
}

//权重函数值
double fx(Variable variable, Weight wight){
    return ( variable.x0 * wight.w[0] )
         + ( variable.x1 * wight.w[1] )
         + ( variable.x2 * wight.w[2] );
}

//迭代更新权重，即：
//w=w+(eta)yx
//b=b+(eta)y
Weight UpdateWeight(Variable variable, Weight weight){
    Weight newWeight;
    newWeight.w[0] = weight.w[0] + eta*( variable.x0 * variable.label );
    newWeight.w[1] = weight.w[1] + eta*( variable.x1 * variable.label );
    newWeight.w[2] = weight.w[2] + eta*( variable.x2 * variable.label );
    return newWeight;
}

int main(){
    //储存的特征变量向量
    vector<Variable> ivec;
    
    //输入的特征变量临时向量
    Variable temp;
    
//    cout << "Please input the dimension:" << endl;
    
    //输入维度
//    cin >> n;
    cout << "Please input “x1,x2,label(2维x数据)”：" << endl;
    
    //输入特征变量 x1,...,xn,符号 label
    while( cin >> temp.x1 >> temp.x2 >> temp.label){
        temp.x0 = 1; //零维，即常数项 w0
        ivec.push_back(temp);//push_back()表示 在Vector最后添加一个元素（参数为要插入的值）
    }
    Weight wit = Wit0;//定义一个权重向量，并且初始化
    
    //iterator 是C++的迭代器，是一种能够实现 “检查容器内元素并遍历之” 的数据类型
    //由于iterator只从头到尾遍历一遍，所以需要 if 中的第二条语句
    for(vector<Variable>::iterator iter = ivec.begin(); iter!=ivec.end(); ++iter){
        if( (*iter).label != sign( fx(*iter,wit) ) ){
            wit = UpdateWeight(*iter, wit);
            iter = ivec.begin();
            //在从头开始判断，因为更新权重后可能会导致前面的点出故障，需要从头再判断
        }
    }
    //打印结果
    cout << "b=" << wit.w[0] << " " << "w1=" << wit.w[1] << " " << "w2=" << wit.w[2] << " " << endl;

}
```

**运行结果：**

<img src="https://cdn.jsdelivr.net/gh/KevinHaoo/PicCloud/000.png" alt="" style="zoom:33%;" />      



## 五、验证



## 六、优缺点

优点：

1. 简单易懂，方便实现。
2. 二分类问题处理效果好。

缺点：

1. 只能处理线性可分的数据。
2. 实际运用中，由于判断的层数太少，常常被弃用。
3. 无法解决回归问题。

## 七、感知机结合神经网络

​        感知机是神经网络（深度学习）的起源算法，学习感知机的构造是通向神经网络和深度学习的一种重要思想。

​        严格讲，应该称为“人工神经元”或“朴素感知机”，但是因为很多基本的处理都是共通的，所以这里就简单地称为“感知机”。

​        感知机接收多个输入信号，输出一个信号。这里所说的“信号”可以想 象成电流或河流那样具备“流动性”的东西。像电流流过导线，向前方输送 电子一样，感知机的信号也会形成流，向前方输送信息。

​        但是，和实际的电流不同的是：感知机的信号只有“流/不流”（1/-1）两种取值，1对应“传递信号”，-1对应“不传递信号”。$x_i$ 是输入信号，$y_i$ 是输出信号，$w_i$ 是权重。下图中，圆圈称为“神经元”或者“节点”。

<img src="https://cdn.jsdelivr.net/gh/KevinHaoo/PicCloud/001.png" alt="" style="zoom:33%;" />      

​        对于两者的进一步、深层次联系，已有前辈进行了很好的总结：[浅谈感知机与神经网络（无师自通）](http://c.biancheng.net/view/1910.html)。

## 七、思考与推广：

1、感知机基础用处在于，能够对于已经分类好的**两类**数据，**寻到一个将其分割的权重函数，在几何角度是找到一个超平面可以将数据点进行两类分割**。

​      而这个权重函数的求出的意义在于，如果又有新的同类型、同维数数据集，那么就可以在此基础上，对新数据应该归为已有两类别中的哪一类进行预测。

​      举个实际些的例子，我们对新冠肺炎的病人可以基于感知机进行**是否感染最新的德尔塔毒株**判断。大致思路是，将新冠肺炎病人中，感染普通毒株和感染德尔塔毒株后，出现**明显不同的症状**进行记录，将这些症状作为**特征变量**，将已有数据作为训练集训练出一个**权重函数**，那么这个权重函数就可以对这个问题进行**新数据集的分类判别**。而此举意义在于，能够快速高效地判断出病人是否感染德尔塔病毒，从而避免因误判而导致常规治疗，耽误治疗时机。



2、对于感知机的改进算法---Pocket Algorithm算法，对于实际问题中的数据集，我们对能否找到那个分割的权重函数并不清楚，因此选用这个算法是比较保守的，虽然运行时间堪忧，但是理论上还是可以出结果的。因此，这个算法在实际情况下有着较好的实用性。



3、**感知机对于神经网络（深度学习）来说，是后者的基石。**

​      我们建立神经网络的思想来源，便是我们已归纳出的感知机思想。在感知机的基础上，当我们需要处理的数据需要经过**多次权重函数的“分类”**时，通过**感知机的叠加组合**，理论上是可以实现的。由此，有了**多层感知机**的概念。而多层感知机形成的一套处理系统，后来就成为了神经网络的雏形。

​      对于第一点联想到的例子来说，现实中往往也是不能单单通过一层感知机来得出结论的，需要选用多层感知机组成的神经网络系统来进行处理评判。



### Reference：

[1] [感知机原理（Perceptron）](https://www.cnblogs.com/huangyc/p/9706575.html)

[2] [PLA算法总结——Percetron Learning Algorithm(机器学习基石2)_](https://blog.csdn.net/u013455341/article/details/46747343?utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-2.no_search_link)

[3] [感知器 - 万维百科](https://wanweibaike.net/wiki-感知器)

[4] [算法——感知机详解（推导+证明）](https://zhuanlan.zhihu.com/p/46762820)

[5] [梯度下降法 —— 经典的优化方法 ](https://zhuanlan.zhihu.com/p/36564434)

[6] [直观理解梯度，以及偏导数、方向导数和法向量等](https://www.cnblogs.com/shine-lee/p/11715033.html)

[7] [PLA算法的C++实现](https://blog.csdn.net/DreamerMonkey/article/details/44080487)

[8] [c++迭代器（iterator）详解](https://www.cnblogs.com/hdk1993/p/4419779.html)

[9] [深度学习入门-感知机 - 简书](https://www.jianshu.com/p/a25788130897)

[10] [浅谈感知机与神经网络（无师自通）](http://c.biancheng.net/view/1910.html)


